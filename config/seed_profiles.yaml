# ============================================================================
# Seed Profiles Configuration
# ============================================================================
#
# This file defines the initial configuration profiles loaded into the database
# when you run: python scripts/init_database.py
#
# ‚ö†Ô∏è IMPORTANT: This file only seeds the INITIAL database state!
#    After initialization, the database is the source of truth.
#    Changes to this file won't affect the running app.
#
# üí° WHEN TO USE THIS FILE:
#    - Setting up a new environment
#    - Creating default profiles for new deployments
#    - Resetting to default configuration (re-run init_database.py)
#
# üí° TO CHANGE SETTINGS IN RUNNING APP:
#    - Use the Settings UI in the app (easiest)
#    - Use the API endpoints (programmatic)
#    - Update database directly (advanced)
#
# ============================================================================

# ----------------------------------------------------------------------------
# Profiles List
# ----------------------------------------------------------------------------
# You can define multiple profiles for different use cases.
# Users can switch between profiles in the UI.
#
# üí° EXAMPLE USE CASES:
#    - Development vs Production (different model endpoints)
#    - Different AI personalities (formal vs casual)
#    - Different capabilities (general vs domain-specific)
#
profiles:
  - name: "Default Chat"
    # Profile name shown in UI
    # üìè Keep it short and descriptive (2-4 words)

    description: "Standard chat assistant with helpful, informative responses"
    # Description shown in profile selection UI
    # üìè One sentence describing the profile's purpose

    is_default: true
    # ‚ö†Ô∏è IMPORTANT: Exactly ONE profile must have is_default: true
    # This profile is auto-selected for new users

    created_by: "system"
    # User who created this profile
    # Use "system" for seed profiles

    # ------------------------------------------------------------------------
    # AI Infrastructure Settings
    # ------------------------------------------------------------------------
    # Configuration for the model serving endpoint and generation parameters
    #
    ai_infra:
      llm_endpoint: "databricks-claude-sonnet-4-5"
      # üéØ THIS IS THE MOST IMPORTANT SETTING
      #
      # The name of your Databricks model serving endpoint.
      # This must match EXACTLY the endpoint name in your Databricks workspace.
      #
      # üìç WHERE TO FIND YOUR ENDPOINT NAME:
      #    1. Log into Databricks workspace
      #    2. Go to: Serving > Serving Endpoints
      #    3. Copy the endpoint name (NOT the URL)
      #
      # ‚ö†Ô∏è COMMON MISTAKES:
      #    ‚ùå Using the full URL (wrong - just need the name)
      #    ‚ùå Typos in the endpoint name (must be exact match)
      #    ‚ùå Using stopped/deleted endpoint (check status is "Ready")
      #
      # ‚úÖ DATABRICKS FOUNDATION MODEL ENDPOINTS (ready to use):
      #    - databricks-meta-llama-3-1-70b-instruct (recommended)
      #    - databricks-meta-llama-3-1-405b-instruct (larger, slower)
      #    - databricks-dbrx-instruct
      #    - databricks-mixtral-8x7b-instruct
      #
      # üí° CUSTOM ENDPOINTS:
      #    You can use any custom model serving endpoint you've deployed:
      #    llm_endpoint: "my-custom-model-endpoint"
      #
      # üîó MORE INFO:
      #    https://docs.databricks.com/machine-learning/foundation-models/

      llm_temperature: 0.7
      # üå°Ô∏è Temperature controls randomness in responses (0.0 - 1.0)
      #
      # üí° HOW IT WORKS:
      #    - 0.0 = Deterministic, predictable (same input ‚Üí same output)
      #    - 0.7 = Balanced creativity (recommended for most cases)
      #    - 1.0 = Maximum creativity, more random
      #
      # üéØ USE CASES:
      #    - 0.0-0.3: Factual Q&A, data extraction, classification
      #    - 0.5-0.7: General chat, customer support (recommended)
      #    - 0.7-1.0: Creative writing, brainstorming, ideation
      #
      # ‚ö†Ô∏è WARNING:
      #    - Too low = Repetitive, boring responses
      #    - Too high = Inconsistent, potentially nonsensical responses

      llm_max_tokens: 2048
      # üìè Maximum number of tokens (words + punctuation) in the response
      #
      # üí° TOKEN ESTIMATION:
      #    - 1 token ‚âà 0.75 words (English)
      #    - 100 tokens ‚âà 75 words ‚âà 1-2 paragraphs
      #    - 500 tokens ‚âà 375 words ‚âà 1 page
      #    - 2048 tokens ‚âà 1500 words ‚âà 3-4 pages
      #
      # üéØ RECOMMENDED VALUES:
      #    - 512: Short answers, quick responses
      #    - 1024: Standard responses (1-2 paragraphs)
      #    - 2048: Detailed explanations (recommended default)
      #    - 4096: Long-form content (if model supports it)
      #
      # ‚ö†Ô∏è IMPORTANT:
      #    - Response will be cut off if it reaches this limit
      #    - Higher values = slower responses and higher cost
      #    - Check your model's maximum token limit
      #    - Most models support 2048-4096 tokens

    # ------------------------------------------------------------------------
    # MLflow Configuration
    # ------------------------------------------------------------------------
    # MLflow tracing for observability and debugging
    #
    mlflow:
      experiment_name: "/Users/{username}/chat-template-experiments"
      # üìä MLflow experiment for storing traces
      #
      # üí° WHAT GETS LOGGED:
      #    - Every chat request/response
      #    - Model parameters used
      #    - Response time and token counts
      #    - Errors and exceptions
      #
      # üìç WHERE TO VIEW TRACES:
      #    Databricks UI > Machine Learning > Experiments > [experiment_name]
      #
      # üéØ NAMING CONVENTIONS:
      #    - Development: "/Users/{username}/[app-name]-dev"
      #    - Staging: "/Users/{username}/[app-name]-staging"
      #    - Production: "/Shared/[app-name]-prod"
      #
      # ‚ö†Ô∏è IMPORTANT:
      #    - {username} is auto-replaced with actual username
      #    - Experiment is auto-created if it doesn't exist
      #    - Use /Shared/ path for team-wide experiments

    # ------------------------------------------------------------------------
    # Prompts Configuration
    # ------------------------------------------------------------------------
    # System prompt and user message template
    #
    prompts:
      system_prompt: |
        You are a helpful AI assistant powered by Databricks. You provide clear,
        accurate, and concise responses to user questions.

        Format your responses using markdown for better readability:
        - Use **bold** for emphasis
        - Use bullet points for lists
        - Use code blocks for code snippets
        - Use headings to organize longer responses

        Be friendly, professional, and helpful.
      # üéØ SYSTEM PROMPT: Defines AI personality and behavior
      #    See config/prompts.yaml for detailed customization guide

      user_prompt_template: "{question}"
      # üìù USER PROMPT TEMPLATE: Wraps each user message
      #    {question} is replaced with actual user input
      #    Usually just "{question}" is fine for most cases

# ============================================================================
# Adding More Profiles
# ============================================================================
#
# You can add multiple profiles for different use cases:
#
# Example: Add a "Code Assistant" profile
#
# - name: "Code Assistant"
#   description: "Specialized assistant for programming questions"
#   is_default: false
#   created_by: "system"
#
#   ai_infra:
#     llm_endpoint: "databricks-meta-llama-3-1-70b-instruct"
#     llm_temperature: 0.3  # Lower for more deterministic code
#     llm_max_tokens: 3072  # More tokens for code examples
#
#   mlflow:
#     experiment_name: "/Users/{username}/code-assistant-experiments"
#
#   prompts:
#     system_prompt: |
#       You are an expert programmer. Help users with:
#       - Writing clean, efficient code
#       - Debugging errors
#       - Explaining programming concepts
#
#       Always include code examples with syntax highlighting.
#     user_prompt_template: "{question}"
#
# ============================================================================
